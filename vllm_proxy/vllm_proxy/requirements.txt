# vLLM Proxy 依赖
# Python 3.8+ required

#==============================================================================
# 核心依赖
#==============================================================================

# FastAPI 框架
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# HTTP 客户端
aiohttp>=3.9.0

# 配置解析
pyyaml>=6.0.1

# GPU 监控
pynvml>=11.5.0

#==============================================================================
# vLLM (核心推理引擎)
# 根据 CUDA 版本选择合适的 vLLM 版本
#==============================================================================

# CUDA 12.1
vllm>=0.2.0

# CUDA 11.8 用户请使用：
# vllm>=0.2.0 --extra-index-url https://download.pytorch.org/whl/cu118

#==============================================================================
# 可选依赖
#==============================================================================

# 性能监控
prometheus-client>=0.19.0

# 请求验证
pydantic>=2.5.0

# 日志处理
python-json-logger>=2.0.7

# 开发调试（可选）
# ipython>=8.18.0
# pytest>=7.4.3
# pytest-asyncio>=0.21.1
