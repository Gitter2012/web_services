# vLLM Proxy 配置文件
# 所有模型均使用 AWQ 量化
# 详细说明请参考 docs/CONFIGURATION.md

#==============================================================================
# GPU 配置
#==============================================================================
gpu:
  # 使用的 GPU ID
  gpu_id: 0

  # 预留显存缓冲 (MB)
  # 用于系统开销和突发请求，建议至少预留 2-4GB
  # vLLM 启动时会从总显存中减去此预留量
  reserved_memory_mb: 1024

  # vLLM 显存利用率 (0.0 - 1.0)
  # 控制 vLLM 进程使用「可用显存」的比例
  # 实际显存使用 = (总显存 - 预留显存) × memory_utilization
  # 例如: (15360 - 1024) × 0.95 = 13619MB
  memory_utilization: 0.95

#==============================================================================
# 代理服务配置
#==============================================================================
proxy:
  # 监听地址
  host: "0.0.0.0"

  # 代理服务端口
  port: 8081

  # vLLM 进程起始端口
  # 每个模型会占用一个端口，从 base_port 开始递增
  base_port: 8000

  # 空闲超时时间 (秒)
  # 模型空闲超过此时间后自动卸载
  idle_timeout_seconds: 300

  # 健康检查间隔 (秒)
  health_check_interval: 10

  # 启动失败最大重试次数
  max_start_retries: 3

  # 模型启动超时时间 (秒)
  start_timeout_seconds: 120

  # 模型停止超时时间 (秒)
  stop_timeout_seconds: 30

  # API Key 认证配置（可选，与 vLLM/OpenAI 兼容格式）
  # 配置后所有请求都需要在 Header 中提供: Authorization: Bearer <api_key>
  # api_key: "your-secret-api-key"

#==============================================================================
# 日志配置
#==============================================================================
logging:
  # 日志级别: DEBUG, INFO, WARNING, ERROR
  level: "INFO"

  # 日志格式
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # 日志文件路径 (相对于项目根目录)
  file: "logs/vllm_proxy.log"

  # 单个日志文件最大大小 (字节)
  max_bytes: 10485760  # 10MB

  # 保留的日志文件数量
  backup_count: 5

#==============================================================================
# 模型配置（所有模型使用 AWQ 量化）
# AWQ 量化模型需要：
#   1. quantization: "awq"
#   2. enforce_eager: true（部分模型需要）
#   3. precision: "fp16" 或 "bf16"
#==============================================================================
models:
  #--------------------------------------------------------------------------
  # Qwen3-3B-AWQ（轻量级模型）
  #--------------------------------------------------------------------------
  qwen3-4b-awq:
    model_path: "Qwen/Qwen3-4B-AWQ"
    param_count: 3
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 16
    enforce_eager: true
    num_layers: 36
    hidden_size: 2048
    num_attention_heads: 16
    num_kv_heads: 8
    explicit_memory_mb: 4000
    extra_args:
      - "--trust-remote-code"

  #--------------------------------------------------------------------------
  # Qwen3-14B-AWQ
  #--------------------------------------------------------------------------
  qwen3-14b-awq:
    model_path: "Qwen/Qwen3-14B-AWQ"
    param_count: 14
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 4096
    max_num_seqs: 8
    enforce_eager: true
    num_layers: 40
    hidden_size: 5120
    num_attention_heads: 40
    num_kv_heads: 8
    explicit_memory_mb: 10000
    extra_args:
      - "--trust-remote-code"

  #--------------------------------------------------------------------------
  # Qwen2.5-7B-AWQ
  #--------------------------------------------------------------------------
  qwen2.5-7b-awq:
    model_path: "Qwen/Qwen2.5-7B-AWQ"
    param_count: 7
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 16
    enforce_eager: true
    num_layers: 28
    hidden_size: 3584
    num_attention_heads: 28
    num_kv_heads: 4
    explicit_memory_mb: 6000
    extra_args:
      - "--trust-remote-code"

  #--------------------------------------------------------------------------
  # Qwen2.5-14B-AWQ
  #--------------------------------------------------------------------------
  qwen2.5-14b-awq:
    model_path: "Qwen/Qwen2.5-14B-AWQ"
    param_count: 14
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 8
    enforce_eager: true
    num_layers: 48
    hidden_size: 5120
    num_attention_heads: 40
    num_kv_heads: 8
    explicit_memory_mb: 10000
    extra_args:
      - "--trust-remote-code"

  #--------------------------------------------------------------------------
  # Qwen2.5-32B-AWQ
  #--------------------------------------------------------------------------
  qwen2.5-32b-awq:
    model_path: "Qwen/Qwen2.5-32B-AWQ"
    param_count: 32
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 4
    enforce_eager: true
    num_layers: 64
    hidden_size: 5120
    num_attention_heads: 40
    num_kv_heads: 8
    explicit_memory_mb: 20000
    extra_args:
      - "--trust-remote-code"

  #--------------------------------------------------------------------------
  # Llama-3.1-8B-AWQ
  #--------------------------------------------------------------------------
  llama3.1-8b-awq:
    model_path: "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
    param_count: 8
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 16
    enforce_eager: false
    num_layers: 32
    hidden_size: 4096
    num_attention_heads: 32
    num_kv_heads: 8
    explicit_memory_mb: 6000

  #--------------------------------------------------------------------------
  # Mistral-7B-AWQ
  #--------------------------------------------------------------------------
  mistral-7b-awq:
    model_path: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
    param_count: 7
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 16
    enforce_eager: false
    num_layers: 32
    hidden_size: 4096
    num_attention_heads: 32
    num_kv_heads: 8
    explicit_memory_mb: 5000

  #--------------------------------------------------------------------------
  # Yi-1.5-9B-AWQ
  #--------------------------------------------------------------------------
  yi-1.5-9b-awq:
    model_path: "TheBloke/Yi-1.5-9B-Chat-AWQ"
    param_count: 9
    precision: "fp16"
    quantization: "awq"
    tensor_parallel: 1
    max_model_len: 8192
    max_num_seqs: 16
    enforce_eager: false
    num_layers: 48
    hidden_size: 4096
    num_attention_heads: 32
    num_kv_heads: 4
    explicit_memory_mb: 7000

  #--------------------------------------------------------------------------
  # DeepSeek-V2-Lite-AWQ（大模型示例，需要更多显存）
  #--------------------------------------------------------------------------
  # deepseek-v2-lite-awq:
  #   model_path: "deepseek-ai/DeepSeek-V2-Lite-Chat-AWQ"
  #   param_count: 16
  #   precision: "fp16"
  #   quantization: "awq"
  #   tensor_parallel: 1
  #   max_model_len: 4096
  #   max_num_seqs: 4
  #   enforce_eager: true
  #   num_layers: 27
  #   hidden_size: 2048
  #   num_attention_heads: 16
  #   num_kv_heads: 2
  #   explicit_memory_mb: 12000
  #   extra_args:
  #     - "--trust-remote-code"
